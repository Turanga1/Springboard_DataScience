{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>topic</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>target_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>topic2_target</th>\n",
       "      <th>...</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "      <th>char_len</th>\n",
       "      <th>word_count</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>computers</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1875</td>\n",
       "      <td>351</td>\n",
       "      <td>[Dear, local, newspaper, ,, I, think, effects,...</td>\n",
       "      <td>[dear, local, newspaper, ,, -PRON-, think, eff...</td>\n",
       "      <td>[ADJ, ADJ, NOUN, PUNCT, PRON, VERB, NOUN, NOUN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>computers</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2288</td>\n",
       "      <td>424</td>\n",
       "      <td>[Dear, @CAPS1, @CAPS2, ,, I, believe, that, us...</td>\n",
       "      <td>[dear, @caps1, @caps2, ,, -PRON-, believe, tha...</td>\n",
       "      <td>[ADJ, PROPN, PUNCT, PUNCT, PRON, VERB, ADP, VE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>computers</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1541</td>\n",
       "      <td>284</td>\n",
       "      <td>[Dear, ,, @CAPS1, @CAPS2, @CAPS3, More, and, m...</td>\n",
       "      <td>[dear, ,, @caps1, @caps2, @caps3, more, and, m...</td>\n",
       "      <td>[ADJ, PUNCT, PROPN, PUNCT, PROPN, ADJ, CCONJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>computers</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3165</td>\n",
       "      <td>531</td>\n",
       "      <td>[Dear, Local, Newspaper, ,, @CAPS1, I, have, f...</td>\n",
       "      <td>[dear, local, newspaper, ,, @caps1, -PRON-, ha...</td>\n",
       "      <td>[ADJ, PROPN, PROPN, PUNCT, PROPN, PRON, VERB, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>computers</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2569</td>\n",
       "      <td>474</td>\n",
       "      <td>[Dear, @LOCATION1, ,, I, know, having, compute...</td>\n",
       "      <td>[dear, @location1, ,, -PRON-, know, have, comp...</td>\n",
       "      <td>[ADJ, ADP, PUNCT, PRON, VERB, VERB, NOUN, VERB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id      topic                                              essay  \\\n",
       "0         1  computers  Dear local newspaper, I think effects computer...   \n",
       "1         2  computers  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3  computers  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4  computers  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5  computers  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  target_score  \\\n",
       "0               4               4             NaN             8   \n",
       "1               5               4             NaN             9   \n",
       "2               4               3             NaN             7   \n",
       "3               5               5             NaN            10   \n",
       "4               4               4             NaN             8   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  topic2_target  \\\n",
       "0             NaN             NaN            NaN   \n",
       "1             NaN             NaN            NaN   \n",
       "2             NaN             NaN            NaN   \n",
       "3             NaN             NaN            NaN   \n",
       "4             NaN             NaN            NaN   \n",
       "\n",
       "                         ...                          rater3_trait2  \\\n",
       "0                        ...                                    NaN   \n",
       "1                        ...                                    NaN   \n",
       "2                        ...                                    NaN   \n",
       "3                        ...                                    NaN   \n",
       "4                        ...                                    NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  char_len  \\\n",
       "0            NaN            NaN            NaN            NaN      1875   \n",
       "1            NaN            NaN            NaN            NaN      2288   \n",
       "2            NaN            NaN            NaN            NaN      1541   \n",
       "3            NaN            NaN            NaN            NaN      3165   \n",
       "4            NaN            NaN            NaN            NaN      2569   \n",
       "\n",
       "   word_count                                             tokens  \\\n",
       "0         351  [Dear, local, newspaper, ,, I, think, effects,...   \n",
       "1         424  [Dear, @CAPS1, @CAPS2, ,, I, believe, that, us...   \n",
       "2         284  [Dear, ,, @CAPS1, @CAPS2, @CAPS3, More, and, m...   \n",
       "3         531  [Dear, Local, Newspaper, ,, @CAPS1, I, have, f...   \n",
       "4         474  [Dear, @LOCATION1, ,, I, know, having, compute...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [dear, local, newspaper, ,, -PRON-, think, eff...   \n",
       "1  [dear, @caps1, @caps2, ,, -PRON-, believe, tha...   \n",
       "2  [dear, ,, @caps1, @caps2, @caps3, more, and, m...   \n",
       "3  [dear, local, newspaper, ,, @caps1, -PRON-, ha...   \n",
       "4  [dear, @location1, ,, -PRON-, know, have, comp...   \n",
       "\n",
       "                                                 pos  \n",
       "0  [ADJ, ADJ, NOUN, PUNCT, PRON, VERB, NOUN, NOUN...  \n",
       "1  [ADJ, PROPN, PUNCT, PUNCT, PRON, VERB, ADP, VE...  \n",
       "2  [ADJ, PUNCT, PROPN, PUNCT, PROPN, ADJ, CCONJ, ...  \n",
       "3  [ADJ, PROPN, PROPN, PUNCT, PROPN, PRON, VERB, ...  \n",
       "4  [ADJ, ADP, PUNCT, PRON, VERB, VERB, NOUN, VERB...  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pd.read_pickle('training_set.pkl')\n",
    "\n",
    "\n",
    "# for clarity, rename numbered essay topics to one-word topic summary \n",
    "\n",
    "topic_dict = {'topic':{1: 'computers', \n",
    "                       2: 'censorship', \n",
    "                       3: 'cyclist', \n",
    "                       4: 'hibiscus', \n",
    "                       5: 'mood', \n",
    "                       6: 'dirigibles', \n",
    "                       7: 'patience', \n",
    "                       8: 'laughter'}}\n",
    "\n",
    "docs.replace(topic_dict, inplace=True)\n",
    "\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetching the text...\n",
      "\n",
      "Preparing the sentences...\n",
      "Num sentences: 7200\n"
     ]
    }
   ],
   "source": [
    "print('\\nFetching the text...')\n",
    "url = 'https://raw.githubusercontent.com/maxim5/stanford-tensorflow-tutorials/master/data/arxiv_abstracts.txt'\n",
    "path = get_file('arxiv_abstracts.txt', origin=url)\n",
    "\n",
    "print('\\nPreparing the sentences...')\n",
    "max_sentence_len = 40\n",
    "with open(path) as file_:\n",
    "  docs = file_.readlines()\n",
    "sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in docs]\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "essays = docs[((docs.topic == 'computers') &\n",
    "            (docs.target_score > 6)) |\n",
    "            ((docs.topic == 'censorship') & \n",
    "            (docs.target_score > 2))]\\\n",
    "            ['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num sentences: 3251\n"
     ]
    }
   ],
   "source": [
    "max_sentence_len = 40\n",
    "# sentences = [[word for word in doc.lower().translate(string.punctuation).split()[:max_sentence_len]] for doc in essays]\n",
    "sentences = [[word.lower() for word in doc[:max_sentence_len]] for doc in essays]\n",
    "print('Num sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec...\n",
      "Result embedding shape: (5065, 100)\n",
      "Checking similar words:\n",
      "  computer -> computers (0.69), internet (0.44), effect (0.35), computors (0.33), biggest (0.32), site (0.32), time (0.31), technology (0.31)\n",
      "  library -> libraries (0.67), libary (0.63), librarie (0.39), book (0.39), libray (0.35), walk (0.35), parent (0.35), stacked (0.34)\n",
      "  book -> movie (0.51), material (0.50), magazine (0.48), books (0.46), story (0.43), somthing (0.42), something (0.41), song (0.41)\n",
      "  learn -> talk (0.61), connect (0.51), communicate (0.47), teach (0.46), explore (0.44), give (0.43), faraway (0.42), informs (0.41)\n",
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (3251, 40)\n",
      "train_y shape: (3251,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rujjn\\Anaconda3\\envs\\capstone2\\lib\\site-packages\\ipykernel\\__main__.py:3: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\rujjn\\Anaconda3\\envs\\capstone2\\lib\\site-packages\\ipykernel\\__main__.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining word2vec...')\n",
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, emdedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)\n",
    "print('Checking similar words:')\n",
    "for word in ['computer', 'library', 'book', 'learn']:\n",
    "  most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in word_model.most_similar(word)[:8])\n",
    "  print('  %s -> %s' % (word, most_similar))\n",
    "\n",
    "def word2idx(word):\n",
    "  return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "  return word_model.wv.index2word[idx]\n",
    "\n",
    "print('\\nPreparing the data for LSTM...')\n",
    "train_x = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "train_y = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "  for t, word in enumerate(sentence[:-1]):\n",
    "    train_x[i, t] = word2idx(word)\n",
    "  train_y[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', train_x.shape)\n",
    "print('train_y shape:', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n"
     ]
    }
   ],
   "source": [
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=emdedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  if temperature <= 0:\n",
    "    return np.argmax(preds)\n",
    "  preds = np.asarray(preds).astype('float64')\n",
    "  preds = np.log(preds) / temperature\n",
    "  exp_preds = np.exp(preds)\n",
    "  preds = exp_preds / np.sum(exp_preds)\n",
    "  probas = np.random.multinomial(1, preds, 1)\n",
    "  return np.argmax(probas)\n",
    "\n",
    "def generate_next(text, num_generated=20):\n",
    "  word_idxs = [word2idx(word) for word in text.lower().split()]\n",
    "  for i in range(num_generated):\n",
    "    prediction = model.predict(x=np.array(word_idxs))\n",
    "    idx = sample(prediction[-1], temperature=0.7)\n",
    "    word_idxs.append(idx)\n",
    "  return ' '.join(idx2word(idx) for idx in word_idxs)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "  print('\\nGenerating text after epoch: %d' % epoch)\n",
    "  texts = [\n",
    "    'dear local newspaper every library should have these books',\n",
    "    'books in library',\n",
    "    'a computer',\n",
    "    'a',\n",
    "  ]\n",
    "  for text in texts:\n",
    "    gen_text = generate_next(text)\n",
    "    print('%s... -> %s' % (text, gen_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3251/3251 [==============================] - 4s 1ms/step - loss: 6.3666\n",
      "\n",
      "Generating text after epoch: 0\n",
      "dear local newspaper every library should have these books... -> dear local newspaper every library should have these books next sir precious shealf of a think stimulate migrated els overheard humor blaring dust older offisive sister happen wish faults\n",
      "books in library... -> books in library Â“cyberspaceÂ” are yes interesting someone hope days neggative doesnt't lines puplic carefully horrendous co thake discussion involve distribution less appauling\n",
      "a computer... -> a computer wirters magzine await obese upheld @organization3 beleave faw documents constitution harm centainly mails ete projected opportunities unnecessary chairs wirting certainly\n",
      "a... -> a swell world hassle incorporated offeneded active displayed portrails notify consern els crude violance conversations beauty support @location2 appauling tortured recieving\n",
      "Epoch 2/5\n",
      "3251/3251 [==============================] - 3s 990us/step - loss: 5.3676\n",
      "\n",
      "Generating text after epoch: 1\n",
      "dear local newspaper every library should have these books... -> dear local newspaper every library should have these books mission leaders-- tht setback north decades humans single puplic knawed rape relatively controversial quarter rid sunny than i oppinion short\n",
      "books in library... -> books in library hook arcross reds art terms communicate magzine libraries!do lives horses own strongly hopefully posible fortunate opinons magizine most argue chance\n",
      "a computer... -> a computer geuss doors boxes us hid english tyoes ponders librares findings propose differant shelve sholud rush alows starts lucky actually useful\n",
      "a... -> a 's enjoy the ... graphis tunes harmfully minding thers alarm draws geuss consumers lack with own accordingly launguage worldwide tutorial\n",
      "Epoch 3/5\n",
      "3251/3251 [==============================] - 3s 1ms/step - loss: 4.8009\n",
      "\n",
      "Generating text after epoch: 2\n",
      "dear local newspaper every library should have these books... -> dear local newspaper every library should have these books offensive creator reactions @caps2.v. actions softwares views proves @caps10 ponders room ever bookshelf guardian reson wear marvels foregin computer talkking\n",
      "books in library... -> books in library lead expressed sign benefil freedoms fight teen instant probally persuasive stories excercise staring weell mostly to now forgot stating what\n",
      "a computer... -> a computer reasons informational varities menial comfy despised debated libary hearing letter slould apart contributed mission moody hurts considered suprised immediately divices\n",
      "a... -> a eye like or the like where others shevles telephone werewolves when probley was -in-@caps2 lock nation liver addition dear havar\n",
      "Epoch 4/5\n",
      "3251/3251 [==============================] - 3s 1ms/step - loss: 4.1998\n",
      "\n",
      "Generating text after epoch: 3\n",
      "dear local newspaper every library should have these books... -> dear local newspaper every library should have these books fiction reffering this discussion exactly choosing contact affend battle trouble prominent evaluating relatively research tooken publically occurs                                                                                                      step required\n",
      "books in library... -> books in library cetian innapropriate twilight answered coordibates publick possibilty frist ownership not programs leave ten sexist so much letter inteligent many spent\n",
      "a computer... -> a computer magazine or or be offensive bitter grately advancements novle showing souldnt does nain throughout works censhorship neglect readings religous beinging\n",
      "a... -> a . only prevent for people should deprives school bileve sign overuse photo novels promote preventing improve searched political intitle involved\n",
      "Epoch 5/5\n",
      "3251/3251 [==============================] - 3s 1ms/step - loss: 3.5670\n",
      "\n",
      "Generating text after epoch: 4\n",
      "dear local newspaper every library should have these books... -> dear local newspaper every library should have these books form picking spell around the do scientis thinkers round indeed spent earth grow back danger at stayed capture theses town\n",
      "books in library... -> books in library 's only own else benefical placs benift front details apropreate peaceful quiet deear next shocking signed starting discussing gender absolutley\n",
      "a computer... -> a computer should camps she elementary out twelve sudden adays gone unjust a , not but around donÂ’@caps2 notebooks stimulate completing playing\n",
      "a... -> a . the or or the we think kkk                         cussing grusome milk stay it try discover fails greately compelled moagazines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x205417dc828>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone2]",
   "language": "python",
   "name": "conda-env-capstone2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
